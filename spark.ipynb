{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"HADOOP_INSTALL\"] = \"/home/hadoop/hadoop\"\n",
    "os.environ[\"HADOOP_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_MAPRED_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_COMMON_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_HDFS_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_YARN_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = os.path.join(os.environ[\"HADOOP_INSTALL\"], \"/etc/hadoop\")\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hadoop/spark\"\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], \"python\"))\n",
    "sys.path.append(os.path.join(os.environ[\"SPARK_HOME\"], \"python/lib/py4j-0.10.9.2-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.121.62:4044\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HelloLines\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromHDFS(filePath):\n",
    "   try:\n",
    "      return spark.read.csv(filePath)\n",
    "   except Exception as e:\n",
    "      return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"hdfs:/datasets/spotify/tracks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[album_name: string, album_uri: string, artist_name: string, artist_uri: string, duration_ms: bigint, pid: bigint, pos: bigint, track_name: string, track_uri: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: json.loads(x))\n",
    "dfp = rdd2.toDF()\n",
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Statistics about songs duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 - Generate a table containing the minimum, average and maximum duration, in milliseconds, of the songs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+\n",
      "|_c0|               _c1|     _c2|\n",
      "+---+------------------+--------+\n",
      "|  0|234408.54976216817|10435467|\n",
      "+---+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregates = getDataFromHDFS('q1.1.csv')\n",
    "if aggregates == -1:\n",
    "    min = dfp.agg({ 'duration_ms' : 'min' })\n",
    "    avg = dfp.agg({ 'duration_ms' : 'avg' })\n",
    "    max = dfp.agg({ 'duration_ms' : 'max' })\n",
    "    aggregates = min.join(avg)\n",
    "    aggregates = aggregates.join(max)\n",
    "    aggregates.write.csv('./q1.1.csv')\n",
    "aggregates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 - Compute the first and third quartiles (denoted Q1​ and Q3​), as well as the interquartile range (IRQ) (Q3​−Q1​)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(198333.0, 258834.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = dfp.approxQuantile(\"duration_ms\", [0.25, 0.75], 0)\n",
    "q1, q3 = quantiles[0], quantiles[1]\n",
    "q1, q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 - Compute the set of songs with durations that are not outliers, as defined by the IQRR methodology. In other words, identify all songs with duration xx such that Q1 − 1.5 × IQR < x < Q3 + 1.5 × IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def iqr_outlier_treatment(df, columns, factor=1.5):\n",
    "    \"\"\"\n",
    "    Detects and treats outliers using IQR for multiple variables in a PySpark DataFrame.\n",
    "\n",
    "    :param dataframe: The input PySpark DataFrame\n",
    "    :param columns: A list of columns to apply IQR outlier treatment\n",
    "    :param factor: The IQR factor to use for detecting outliers (default is 1.5)\n",
    "    :return: The processed DataFrame with outliers treated\n",
    "    \"\"\"\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = q1 - factor * iqr\n",
    "    upper_bound = q3 + factor * iqr\n",
    "    \n",
    "    for column in columns:\n",
    "        # Filter outliers and no outliers\n",
    "        df_no_outliers = df.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "        df_outliers = df.filter((col(column) <= lower_bound) & (col(column) >= upper_bound))\n",
    "        #df_no_outliers, df_outliers\n",
    "\n",
    "    return df_no_outliers, df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataFrame[album_name: string, album_uri: string, artist_name: string, artist_uri: string, duration_ms: bigint, pid: bigint, pos: bigint, track_name: string, track_uri: string],\n",
       " DataFrame[album_name: string, album_uri: string, artist_name: string, artist_uri: string, duration_ms: bigint, pid: bigint, pos: bigint, track_name: string, track_uri: string])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not_outliers\n",
    "iqr_outlier_treatment(dfp, dfp.columns)\n",
    "#df_treated\n",
    "#df_no_outliers = df_treated[0]\n",
    "\n",
    "#df_no_outliers.count()\n",
    "\n",
    "#df_no_outliers\n",
    "\n",
    "# aggregates = getDataFromHDFS('q1.3.csv')\n",
    "# if aggregates == -1:\n",
    "#     min = df_no_outliers.agg({ 'duration_ms' : 'min' })\n",
    "#     avg = df_no_outliers.agg({ 'duration_ms' : 'avg' })\n",
    "#     max = df_no_outliers.agg({ 'duration_ms' : 'max' })\n",
    "#     aggregates = min.join(avg)\n",
    "#     aggregates = aggregates.join(max)\n",
    "#     aggregates.write.csv('./q1.3.csv')\n",
    "# aggregates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 - Using the IQRR methodology, how many songs would be considered outliers and removed from analysis? Generate a new table containing the minimum, average and maximum duration of the remaining songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers\n",
    "df_outliers = df_treated[1]\n",
    "\n",
    "df_outliers.count()\n",
    "\n",
    "aggregates = getDataFromHDFS('q1.4.csv')\n",
    "if aggregates == -1:\n",
    "    min = df_outliers.agg({ 'duration_ms' : 'min' })\n",
    "    avg = df_outliers.agg({ 'duration_ms' : 'avg' })\n",
    "    max = df_outliers.agg({ 'duration_ms' : 'max' })\n",
    "    aggregates = min.join(avg)\n",
    "    aggregates = aggregates.join(max)\n",
    "    aggregates.write.csv('./q1.4.csv')\n",
    "aggregates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Playlists's behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more common, playlists where there are many songs by the same artist or playlists with more diverse songs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlists com mais musicas do mesmo artista\n",
    "\n",
    "\n",
    "\n",
    "# playlists com mais musicas de artistas variados\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
