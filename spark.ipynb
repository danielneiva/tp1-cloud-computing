{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"HADOOP_INSTALL\"] = \"/home/hadoop/hadoop\"\n",
    "os.environ[\"HADOOP_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_MAPRED_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_COMMON_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_HDFS_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_YARN_HOME\"] = os.environ[\"HADOOP_INSTALL\"]\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = os.path.join(os.environ[\"HADOOP_INSTALL\"], \"/etc/hadoop\")\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/hadoop/spark\"\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], \"python\"))\n",
    "sys.path.append(os.path.join(os.environ[\"SPARK_HOME\"], \"python/lib/py4j-0.10.9.2-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 13:02:55,354 WARN util.Utils: Your hostname, cloud resolves to a loopback address: 127.0.2.1; using 192.168.121.62 instead (on interface eth0)\n",
      "2023-10-08 13:02:55,357 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hadoop/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-08 13:02:56,449 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-10-08 13:02:57,854 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2023-10-08 13:02:57,854 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2023-10-08 13:02:57,855 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "2023-10-08 13:02:57,855 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "2023-10-08 13:02:57,855 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://192.168.121.62:4045\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HelloLines\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1024M\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFromHDFS(filePath):\n",
    "   try:\n",
    "      return spark.read.csv(filePath)\n",
    "   except Exception as e:\n",
    "      return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"hdfs:/datasets/spotify/tracks.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[album_name: string, album_uri: string, artist_name: string, artist_uri: string, duration_ms: bigint, pid: bigint, pos: bigint, track_name: string, track_uri: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.map(lambda x: json.loads(x))\n",
    "dfp = rdd2.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Statistics about songs duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 - Generate a table containing the minimum, average and maximum duration, in milliseconds, of the songs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------+\n",
      "|_c0|               _c1|     _c2|\n",
      "+---+------------------+--------+\n",
      "|  0|234408.54976216817|10435467|\n",
      "+---+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregates = getDataFromHDFS('q1.1.csv')\n",
    "if aggregates == -1:\n",
    "    min = dfp.agg({ 'duration_ms' : 'min' })\n",
    "    avg = dfp.agg({ 'duration_ms' : 'avg' })\n",
    "    max = dfp.agg({ 'duration_ms' : 'max' })\n",
    "    aggregates = min.join(avg)\n",
    "    aggregates = aggregates.join(max)\n",
    "    aggregates.write.csv('./q1.1.csv')\n",
    "aggregates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 - Compute the first and third quartiles (denoted Q1​ and Q3​), as well as the interquartile range (IRQ) (Q3​−Q1​)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(198333.0, 258834.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = dfp.approxQuantile(\"duration_ms\", [0.25, 0.75], 0)\n",
    "q1, q3 = quantiles[0], quantiles[1]\n",
    "q1, q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 - Compute the set of songs with durations that are not outliers, as defined by the IQRR methodology. In other words, identify all songs with duration xx such that Q1 − 1.5 × IQR < x < Q3 + 1.5 × IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_outlier_treatment(df, factor=1.5):\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Define the upper and lower bounds for outliers\n",
    "    lower_bound = q1 - factor * iqr\n",
    "    upper_bound = q3 + factor * iqr\n",
    "\n",
    "    df_not_outliers = df.filter((df[\"duration_ms\"] > lower_bound) & (df[\"duration_ms\"] < upper_bound))\n",
    "    df_outliers = df.subtract(df_not_outliers)\n",
    "\n",
    "    return df_not_outliers, df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10200555"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not_outliers\n",
    "df_treated = iqr_outlier_treatment(dfp)\n",
    "df_not_outliers = df_treated[0]\n",
    "df_not_outliers.count()\n",
    "\n",
    "# not_outliers_aggregates = getDataFromHDFS('q1.3.csv')\n",
    "# if not_outliers_aggregates == -1:\n",
    "#     min = df_not_outliers.agg({ 'duration_ms' : 'min' })\n",
    "#     avg = df_not_outliers.agg({ 'duration_ms' : 'avg' })\n",
    "#     max = df_not_outliers.agg({ 'duration_ms' : 'max' })\n",
    "#     not_outliers_aggregates = min.join(avg)\n",
    "#     not_outliers_aggregates = not_outliers_aggregates.join(max)\n",
    "#     not_outliers_aggregates.write.csv('./q1.3.csv')\n",
    "# not_outliers_aggregates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 - Using the IQRR methodology, how many songs would be considered outliers and removed from analysis? Generate a new table containing the minimum, average and maximum duration of the remaining songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------+\n",
      "|_c0|              _c1|     _c2|\n",
      "+---+-----------------+--------+\n",
      "|  0|371193.3242420833|10435467|\n",
      "+---+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outliers\n",
    "df_outliers = df_treated[1]\n",
    "#df_outliers.count()\n",
    "\n",
    "outliers_aggregates = getDataFromHDFS('q1.4.csv')\n",
    "if outliers_aggregates == -1:\n",
    "    min = df_outliers.agg({ 'duration_ms' : 'min' })\n",
    "    avg = df_outliers.agg({ 'duration_ms' : 'avg' })\n",
    "    max = df_outliers.agg({ 'duration_ms' : 'max' })\n",
    "    outliers_aggregates = min.join(avg)\n",
    "    outliers_aggregates = aggregates.join(max)\n",
    "    outliers_aggregates.write.csv('./q1.4.csv')\n",
    "outliers_aggregates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Playlists's behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more common, playlists where there are many songs by the same artist or playlists with more diverse songs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f833b16e850>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# playlists com mais musicas do mesmo artista vs playlists com mais musicas de artistas variados\n",
    "# tem um atributo com num_artists (the total number of unique artists for the tracks in the playlist)\n",
    "# pegar o top artista mais frequente em cada playlist \n",
    "\n",
    "# groupBy pid da playlist, pelo artista e vê qnts musicas esse artista tem na playlist\n",
    "\n",
    "df_grouped = dfp.groupBy('pid', 'artist_name')\n",
    "\n",
    "# prevalence = total de musicas/total de musicas do artista mais frequente\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
